<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【ETL】基于Flink SQL CDC+MQ Connect同步数据]]></title>
    <url>%2Fposts%2F70932d1%2F</url>
    <content type="text"><![CDATA[一、代码实现1234567891011121314151617public class MySQLDataToMQ &#123; public static void main(String[] args) throws Exception &#123; // 获取执行环境 StreamExecutionEnvironment executeEnv = EnvironmentUtils.getStreamExecutionEnvironment(); StreamTableEnvironment tableEnv = EnvironmentUtils.getStreamTableEnvironment(); // 从源表同步数据到目标表 tableEnv.executeSql( "INSERT INTO target_kafka_t_regulation_matter SELECT * FROM source_mysql_t_regulation_matter" ).print(); // 提交任务 executeEnv.execute("从【MySQL监管事项源表】同步数据到【Kafka监管事项目标表】"); &#125;&#125; 二、创建Kafka Topic12345# 格式kafka-topics.sh --create --zookeeper $&#123;zookeeperIP&#125;:$&#123;zookeeper端口&#125; --replication-factor 1 --partitions 1 --topic $&#123;Topic-Name&#125;# 示例kafka-topics.sh --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 1 --topic t_regulation_matter-change_log 三、任务提交12345# 提交初始化任务bin/flink run -m 127.0.0.1:8081 -c io.cheery.flink.example.MySQLDataToMQ ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar# 开启任务增量checkpointbin/flink savepoint 943f8665978b9c71b930139dc9910f7a hdfs://127.0.0.1:8082/flink/savepoint 访问Flink管理页面查看任务运行情况（http://127.0.0.1:8081/#/overview） 监听【t_regulation_matter-change_log】topic 12345# 格式kafka-console-consumer.sh --bootstrap-server $&#123;KafkaIP&#125;:$&#123;Kafka端口&#125; --topic $&#123;Topic-Name&#125; --from-beginning# 示例kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic t_regulation_matter-change_log --from-beginning 查看输出 四、业务测试测试一：测试任务意外中断并且MySQL数据数据已发生变化的场景 关闭任务 MySQL数据改变 12345678# 新增INSERT INTO t_regulation_matter ( matter_code,matter_name ) VALUES ('00000000', '测试事项数据');# 修改UPDATE t_regulation_matter SET matter_name='对经营者利用合同格式条款排除消费者权利行为的监管-修改' WHERE id = 4;# 删除DELETE FROM t_regulation_matter WHERE id = 3; 从checkpoint恢复任务 1bin/flink run -m 127.0.0.1:8081 -s hdfs://127.0.0.1:8082/flink/savepoint/savepoint-943f86-5cd41f0130bb -c io.cheery.flink.example.MySQLDataToMQ ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar 查看数据是否丢失 测试二：测试下游接收端Kafka意外中断并且MySQL数据数据已发生变化的场景 关闭Kafka 1docker stop $&#123;容器ID或容器名&#125; MySQL数据改变 12345678# 新增INSERT INTO t_regulation_matter ( matter_code,matter_name ) VALUES ('00000000', '测试事项数据-1');# 修改UPDATE t_regulation_matter SET matter_name='对经营者利用合同格式条款排除消费者权利行为的监管-修改-再次修改' WHERE id = 4;# 删除DELETE FROM t_regulation_matter WHERE id = 2; 开启MySQL 1docker start $&#123;容器ID或容器名&#125; 查看数据是否丢失]]></content>
      <categories>
        <category>ETL</category>
      </categories>
      <tags>
        <tag>flink&amp;mq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】基于Flink SQL CDC+ES Connect同步数据]]></title>
    <url>%2Fposts%2Fb78e417%2F</url>
    <content type="text"><![CDATA[一、代码实现123456789101112131415161718192021222324252627public class MySQLDataToEs &#123; public static void main(String[] args) throws Exception &#123; // 获取执行环境 StreamExecutionEnvironment executeEnv = EnvironmentUtils.getStreamExecutionEnvironment(); StreamTableEnvironment tableEnv = EnvironmentUtils.getStreamTableEnvironment(); // 从源表同步数据到目标表 tableEnv.executeSql("" + " INSERT INTO target_es_regulation_matter_regulation_action_rel " + " SELECT " + " ra.id AS action_id, " + " ra.action_name, " + " rm.matter_code, " + " rm.matter_name, " + " ra.regulation_object_num, " + " ra.regulation_object_name " + " FROM " + " source_mysql_t_regulation_matter rm " + " INNER JOIN source_mysql_t_regulation_action ra ON ra.matter_code = rm.matter_code " ).print(); // 提交任务 executeEnv.execute("从【MySQL监管事项表+监管行为表】同步数据到【ES监管事项监管行为关联目标表】"); &#125;&#125; 二、任务提交 执行命令 12345678# 任务提交格式$&#123;flink安装路径&#125;/flink-1.13.6/bin/flink run -m $&#123;flink地址&#125; -c $&#123;类的全路径&#125; $&#123;服务jar包所在路径&#125;# 案例（进入安装目录）bin/flink run -m 127.0.0.1:8081 -c io.cheery.flink.example.MySQLDataToEs ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar# 出现如下输出结果表示成功Job has been submitted with JobID $&#123;JobID&#125; 访问Flink管理页面查看任务运行情况（http://127.0.0.1:8081/#/overview） 访问ElasticSearch-Head查看ES数据同步情况（http://127.0.0.1:9100） 开启增量checkpoint1234567891011# 任务提交格式$&#123;flink安装路径&#125;/flink-1.13.6/bin/flink savepoint $&#123;JobID&#125; $&#123;hdfs路径&#125;# 案例（进入安装目录）bin/flink savepoint 00d027c2b8c0f1f066bd8edb43ff13d0 hdfs://127.0.0.1:8082/flink/savepoint# 出现如下输出结果表示成功Triggering savepoint for job 00d027c2b8c0f1f066bd8edb43ff13d0.Waiting for response...Savepoint completed. Path: hdfs://127.0.0.1:8082/flink/savepoint/savepoint-00d027-4030cc0f10f8You can resume your program from this savepoint with the run command. 三、功能测试测试一：测试任务意外中断并且MySQL数据数据已发生变化的场景 关闭任务 MySQL数据改变 12345678910# 新增INSERT INTO t_regulation_action ( action_name, matter_code, regulation_object_num, regulation_object_name )VALUES ( '测试数据', '00230004', '测试数据', '测试数据'); # 修改UPDATE t_regulation_action SET regulation_object_name='xx酒店有限公司-修改' WHERE id = 4;# 删除DELETE FROM t_regulation_action WHERE id = 3; 从checkpoint恢复任务 12345678# 任务提交格式$&#123;flink安装路径&#125;/flink-1.13.6/bin/flink run -m $&#123;flink地址&#125; -s $&#123;hdfs对checkpoint保存路径&#125; -c $&#123;类的全路径&#125; $&#123;服务jar包所在路径&#125;# 案例（进入安装目录）bin/flink run -m 127.0.0.1:8081 -s hdfs://127.0.0.1:8082/flink/savepoint/savepoint-00d027-4030cc0f10f8 -c io.cheery.flink.example.MySQLDataToEs ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar# 出现如下输出结果表示成功Job has been submitted with JobID $&#123;JobID&#125; 查看数据是否丢失 测试二：测试下游接收端ElasticSearch意外中断并且MySQL数据数据已发生变化的场景 关闭ElasticSearch 1docker stop $&#123;容器ID或容器名&#125; MySQL数据改变 12345678910# 新增INSERT INTO t_regulation_action ( action_name, matter_code, regulation_object_num, regulation_object_name )VALUES( '测试数据-1', '00230004', '测试数据-1', '测试数据-1');# 修改UPDATE t_regulation_action SET regulation_object_name='xx酒店有限公司-修改-再次修改' WHERE id = 4;# 删除DELETE FROM t_regulation_action WHERE id = 2; 启动ElasticSearch 1docker start $&#123;容器ID或容器名&#125; 查看数据是否丢失]]></content>
      <categories>
        <category>ETL</category>
      </categories>
      <tags>
        <tag>flink&amp;es</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】基于Flink SQL CDC表对表之间同步数据]]></title>
    <url>%2Fposts%2Facc24ab%2F</url>
    <content type="text"><![CDATA[一、代码实现1234567891011121314151617public class MySQLDataToMySQL &#123; public static void main(String[] args) throws Exception &#123; // 获取执行环境 StreamExecutionEnvironment executeEnv = EnvironmentUtils.getStreamExecutionEnvironment(); StreamTableEnvironment tableEnv = EnvironmentUtils.getStreamTableEnvironment(); // 从源表同步数据到目标表 tableEnv.executeSql( "INSERT INTO target_mysql_t_regulation_matter SELECT *, LOCALTIMESTAMP FROM source_mysql_t_regulation_matter" ).print(); // 提交任务 executeEnv.execute("从【MySQL监管事项源表】同步数据到【MySQL监管事项目标表】"); &#125;&#125; 二、任务提交 执行命令 12345# 提交初始化任务bin/flink run -m 127.0.0.1:8081 -c io.cheery.flink.example.MySQLDataToMySQL ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar# 开启任务增量checkpointbin/flink savepoint 4caad952cafe6fe63de0b30058c7c4a2 hdfs://127.0.0.1:8082/flink/savepoint 访问Flink管理页面查看任务运行情况（http://127.0.0.1:8081/#/overview） 访问MySQL可视化工具查看目标数据库的同步情况 三、功能测试测试一：测试任务意外中断并且MySQL数据数据已发生变化的场景 关闭任务 MySQL数据改变 12345678# 新增INSERT INTO t_regulation_matter ( matter_code,matter_name ) VALUES ('00000000', '测试事项数据');# 修改UPDATE t_regulation_matter SET matter_name='对经营者利用合同格式条款排除消费者权利行为的监管-修改' WHERE id = 4;# 删除DELETE FROM t_regulation_matter WHERE id = 3; 从checkpoint恢复任务 1bin/flink run -m 127.0.0.1:8081 -s hdfs://127.0.0.1:8082/flink/savepoint/savepoint-4caad9-50717da00e58 -c io.cheery.flink.example.MySQLDataToMySQL ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar 查看数据是否有丢失 测试二：测试下游接收端MySQL意外中断并且MySQL数据数据已发生变化的场景 关闭MySQL 1docker stop $&#123;容器ID或容器名&#125; MySQL数据改变 12345678# 新增INSERT INTO t_regulation_matter ( matter_code,matter_name ) VALUES ('00000000', '测试事项数据-1');# 修改UPDATE t_regulation_matter SET matter_name='对经营者利用合同格式条款排除消费者权利行为的监管-修改-再次修改' WHERE id = 4;# 删除DELETE FROM t_regulation_matter WHERE id = 2; 开启MySQL 1docker start $&#123;容器ID或容器名&#125; 查看数据是否丢失]]></content>
      <categories>
        <category>ETL</category>
      </categories>
      <tags>
        <tag>flink&amp;mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】Flink SQL CDC实时数据同步实战]]></title>
    <url>%2Fposts%2F7bb813a%2F</url>
    <content type="text"><![CDATA[一、CDC技术方案比较 定时任务+时间戳 Canal Flink SQL CDC CDC机制 查询 日志 日志 全量同步 是 否 是 增量同步 是（实现较麻烦） 是 是 全量+增量同步 否 否 是 断点续传 否 是 是 低延迟+不增加数据库负载 否 是 是 丰富的数据源 是 否（仅支持MySQL） 是（DB、MQ、FS….） 与业务隔离 否 是 是 同数据源多表数据关联结果集同步 是 否（仅支持单表数据） 是 多数据源多表数据关联结果集同步 否 否 是 二、业务需求分析需求一：MySQL中监管事项与监管行为关联出的结果同步到ElasticSearch以提升查询速度（实时性要求严格） 需要展示一张监管事项与监管行为相关联的列表，由于业务的迭代在生产环境中全省的监管事项数据已达到300w左右，监管行为数据已达到1200w左右，在MySQL加索引及其他优化的情况下查询出的结果还是大于5s，对于用户体验非常不好。 最终采用能基于MySQL二进制日志捕获数据变更的框架（Flink SQL CDC）和全文搜索引擎（ElasticSearch）实现。 以下是数据流向图： 需求二：MySQL中监管事项表全量和增量数据同步到不同MySQL数据库中以支撑其他系统业务（实时性要求较严格） 监管事项表存在于监管事项库用作日常对事项数据的维护 监管平台数据库需要结合监管事项数据跟本系统产生的数据进行聚合 1、如根据事项类型分组查询每种事项分类产生的办件被监管了多少 2、如根据事项编制部门分组查询每种事项分类产生的办件被监管了多少 3、N… 以下是数据流向图： 需求三：MySQL中监管事项表全量和增量数据同步MQ供下游业务系统消费到不同的数据存储端（实时性要求较严格） 以下是数据流向图： 三、前置环境准备（与生产环境版本保持一致） Kafka2.7.0 Zookeeper3.5.5 ElasticSearch7.1.1 ElasticSearch-Head MySQL5.7开启binlog 修改配置文件mysqld.cnf123456# 唯一ID（数字类型 主从情况下务必保持各个节点都唯一）server-id=10# 开启binloglog-bin=mysql-bin# 主从复制格式（mixed、statement（默认值）、row）binlog-format=row Flink1.13.6安装 参考 =&gt; https://hironaldo.github.io/posts/e00738f/ 业务支撑依赖下载 MySQL驱动：mysql-connector-java-5.1.47.jar MySQL-CDC：flink-connector-mysql-cdc-2.0.0.jar JDBC-Connector：flink-connector-jdbc_2.12-1.13.6.jar Kafka-Connector：flink-connector-kafka_2.12-1.13.6.jar ElasticSearch-Connector：flink-sql-connector-elasticsearch7_2.12-1.13.6.jar flink-shaded-hadoop-2-uber：flink-shaded-hadoop-2-uber-2.8.3-10.0.jar 将依赖包放入（${flink安装目录}/flink-1.13.6/lib） Hadoop2.10.1安装 参考 =&gt; https://hironaldo.github.io/posts/3de88dc/ Hive2.3.4安装 参考 =&gt; https://hironaldo.github.io/posts/42b1cdd/ 四、表结构初始化（只展示部分关键字段） MySQL监管事项源表 12345678910111213141516171819SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;DROP TABLE IF EXISTS `t_regulation_matter`;CREATE TABLE `t_regulation_matter` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `matter_code` varchar(8) NOT NULL COMMENT '事项编码', `matter_name` varchar(100) NOT NULL COMMENT '事项名称', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4;BEGIN;INSERT INTO `t_regulation_matter` VALUES (1, '00250002', '对相关行业生产经营单位的应急预案的监管');INSERT INTO `t_regulation_matter` VALUES (2, '00230004', '对职业病危害场所、单位和建设项目的监管');INSERT INTO `t_regulation_matter` VALUES (3, '00230034', '对医疗机构抗菌药物临床应用的监管');INSERT INTO `t_regulation_matter` VALUES (4, '00310107', '对经营者利用合同格式条款排除消费者权利行为的监管');COMMIT;SET FOREIGN_KEY_CHECKS = 1; MySQL监管事项目标表2 该表位于数据库2用户接收数据库1源表同步的数据 多了一个sync_time同步时间的字段12345678910111213SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;DROP TABLE IF EXISTS `t_regulation_matter`;CREATE TABLE `t_regulation_matter` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `matter_code` varchar(8) NOT NULL COMMENT '事项编码', `matter_name` varchar(100) NOT NULL COMMENT '事项名称', `sync_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '同步时间', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4;SET FOREIGN_KEY_CHECKS = 1; MySQL监管行为源表 123456789101112131415161718192021SET NAMES utf8mb4;SET FOREIGN_KEY_CHECKS = 0;DROP TABLE IF EXISTS `t_regulation_action`;CREATE TABLE `t_regulation_behavior` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键ID', `action_name` varchar(100) NOT NULL COMMENT '监管行为名称', `matter_code` varchar(8) NOT NULL COMMENT '事项编码', `regulation_object_num` varchar(100) NOT NULL COMMENT '监管对象证件号码', `regulation_object_name` varchar(100) NOT NULL COMMENT '监管对象名称', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4;BEGIN;INSERT INTO `t_regulation_behavior` VALUES (1, '对xx县xx镇xx村卫生室的监督', '00230034', '1232092546848774XN', 'xx村卫生所');INSERT INTO `t_regulation_behavior` VALUES (2, '对xx区xx口腔诊所的监督', '00230034', '92320508MA1PKLXR9W', 'xx口腔诊所');INSERT INTO `t_regulation_behavior` VALUES (3, '对xx区xx幼儿园的监督', '00250002', '91320400467288247U', 'xx幼儿园');INSERT INTO `t_regulation_behavior` VALUES (4, '对xx市xx酒店有限公司的监督', '00250002', '5232090351064181XK', 'xx酒店有限公司');COMMIT;SET FOREIGN_KEY_CHECKS = 1; Flink监管事项目标表 使用mysql-cdc连接器 WITH语句中server-time-zone值必须为Asia/Shanghai123456789101112131415CREATE TABLE source_mysql_t_regulation_matter ( id INT, matter_code STRING, matter_name STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '$&#123;主机名&#125;', 'port' = '$&#123;主机端口&#125;', 'username' = '$&#123;用户名&#125;', 'password' = '$&#123;密码&#125;', 'database-name' = '$&#123;源表所属数据库&#125;', 'table-name' = '$&#123;源表名称&#125;'); Flink监管事项目标表 使用jdbc连接器 url中serverTimezone=Asia/Shanghai不能省略 1234567891011121314CREATE TABLE IF NOT EXISTS target_t_regulation_matter ( id INT, matter_code STRING, matter_name STRING, sync_time TIMESTAMP, PRIMARY KEY (id) NOT ENFORCED)WITH ( 'connector' = 'jdbc', 'url' = '$&#123;连接地址&#125;?serverTimezone=Asia/Shanghai', 'driver' = 'com.mysql.jdbc.Driver', 'username' = '$&#123;用户名&#125;', 'password' = '$&#123;密码&#125;', 'table-name' = '$&#123;源表名称&#125;' ); Flink监管行为目标表 使用mysql-cdc连接器1234567891011121314151617CREATE TABLE source_mysql_t_regulation_action ( id INT, action_name STRING, matter_code STRING, regulation_object_num STRING, regulation_object_name STRING, PRIMARY KEY (id) NOT ENFORCED) WITH ( 'connector' = 'mysql-cdc', 'server-time-zone' = 'Asia/Shanghai', 'hostname' = '$&#123;主机名&#125;', 'port' = '$&#123;主机端口&#125;', 'username' = '$&#123;用户名&#125;', 'password' = '$&#123;密码&#125;', 'database-name' = '$&#123;源表所属数据库&#125;', 'table-name' = '$&#123;源表名称&#125;'); Flink监管事项与监管行为关联的结果目标表 使用elasticsearch-7连接器12345678910111213CREATE TABLE target_es_regulation_matter_regulation_action_rel ( action_id INT, action_name STRING, matter_code STRING, matter_name STRING, regulation_object_num STRING, regulation_object_name STRING, PRIMARY KEY (regulation_behavior_id) NOT ENFORCED) WITH ( 'connector' = 'elasticsearch-7', 'hosts' = '$&#123;elasticsearch地址&#125;', 'index' = '$&#123;elasticsearch索引&#125;'); 五、工具类和Maven依赖 pom.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;!-- 所用版本尽量与flink服务版本一致 --&gt; &lt;flink-version&gt;1.13.6&lt;/flink-version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- Flink --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-json&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Flink-MySQL连接器 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.ververica&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-mysql-cdc&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-jdbc_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Flink-Elasticsearch连接器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-sql-connector-elasticsearch7_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Flink-Kafka连接器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Flink-Hive连接器 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-hive_2.12&lt;/artifactId&gt; &lt;version&gt;$&#123;flink-version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hadoop --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Hive --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;2.3.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- MySQL驱动 --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Log --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.15.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- FastJSON --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.79&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.22&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; EnvironmentUtils.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class EnvironmentUtils &#123; // Hive数据库名称 private static final String HIVE_DATABASE_NAME = "flink_db"; // Hive Catalog名称 private static final String HIVE_CATALOG_NAME = "flink_catalog"; // hdfs存放checkpoint的路径（可从配置文件或配置中心获取） private static final String HDFS_URL = "hdfs://localhost:8082/flink/checkpoint"; // hive-conf路径（可从配置文件或配置中心获取） private static final String HIVE_CONF_PATH = "/Users/ronaldo/Library/ronaldo/develop/hive/apache-hive-2.3.4/conf"; /** * 获取执行环境 */ public static StreamExecutionEnvironment getStreamExecutionEnvironment() &#123; StreamExecutionEnvironment executeEnv = StreamExecutionEnvironment.getExecutionEnvironment(); // 设置并行度为1 executeEnv.setParallelism(1); // 每隔n毫秒进行一次checkpoint executeEnv.enableCheckpointing(5000); // 一次checkpoint超过n毫秒仍未完成直接将其终止以免占用太多资源 executeEnv.getCheckpointConfig().setCheckpointTimeout(60000); // 最大只允许一个checkpoint执行 新的Checkpoint需要挂起等待 executeEnv.getCheckpointConfig().setMaxConcurrentCheckpoints(1); // 保证在重启恢复时 所有算子的状态对任一条数据只处理一次 executeEnv.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE); // 使用hdfs作为State Backend executeEnv.setStateBackend(new FsStateBackend(HDFS_URL)); return executeEnv; &#125; /** * 获取表的执行环境 */ public static StreamTableEnvironment getStreamTableEnvironment() &#123; StreamTableEnvironment tableEnv = StreamTableEnvironment.create( EnvironmentUtils.getStreamExecutionEnvironment(), EnvironmentSettings.newInstance().useBlinkPlanner().build() ); // 注册Hive Catalog tableEnv.registerCatalog( HIVE_CATALOG_NAME, new HiveCatalog(HIVE_CATALOG_NAME, HIVE_DATABASE_NAME, HIVE_CONF_PATH) ); // 使用Hive Catalog tableEnv.useCatalog(HIVE_CATALOG_NAME); return tableEnv; &#125; &#125; TableBuildUtils.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public class TableBuildUtils &#123; /** * MySQL源表模版 */ private static final String MYSQL_SOURCE_TABLE_TEMPLATE = "CREATE TABLE IF NOT EXISTS %s (%s) " + "WITH ( 'connector' = 'mysql-cdc', 'hostname' = '%s', 'port' = '%s', " + "'username' = '%s', 'password' = '%s', 'database-name' = '%s', 'table-name' = '%s', 'server-time-zone' = 'Asia/Shanghai' )"; /** * MySQL目标表模版 */ private static final String MYSQL_TARGET_TABLE_TEMPLATE = "CREATE TABLE IF NOT EXISTS %s (%s)" + "WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://%s:%s/%s?serverTimezone=Asia/Shanghai', 'driver' = 'com.mysql.jdbc.Driver', " + "'username' = '%s', 'password' = '%s', 'table-name' = '%s' )"; /** * ElasticSearch目标表模版 */ private static final String ES_TARGET_TABLE_TEMPLATE = "CREATE TABLE IF NOT EXISTS %s (%s) " + "WITH ( 'connector' = 'elasticsearch-7', 'hosts' = '%s', 'index' = '%s' )"; /** * Kafka目标表模版 */ private static final String KAFKA_TARGET_TABLE_TEMPLATE = "CREATE TABLE IF NOT EXISTS %s (%s) " + "WITH ( 'connector' = 'kafka', 'format' = '%s', 'topic' = '%s', 'properties.bootstrap.servers' = '%s' )"; /** * desc：构建MySQL源表DDL * * @return 完整的建表语句 */ public static String buildMySQLSourceTableDDL(MySQLConfig config) &#123; return String.format( MYSQL_SOURCE_TABLE_TEMPLATE, "source_mysql_" + config.getTableName(), config.getTableSchema(), config.getHostname(), config.getPort(), config.getUsername(), config.getPassword(), config.getDbName(), config.getTableName() ); &#125; /** * desc：批量构建MySQL源表DDL * * @return 完整的建表语句 */ public static List&lt;String&gt; buildBatchMySQLSourceTableDDL(MySQLConfig config) &#123; List&lt;String&gt; sql = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, String&gt; ele : config.getTableInfos().entrySet()) &#123; String tableName = ele.getKey(); String tableSchema = ele.getValue(); sql.add(String.format( MYSQL_SOURCE_TABLE_TEMPLATE, "source_mysql_" + tableName, tableSchema, config.getHostname(), config.getPort(), config.getUsername(), config.getPassword(), config.getDbName(), tableName )); &#125; return sql; &#125; /** * desc：构建MySQL目的表DDL * p.s. url一定要加上serverTimezone=Asia/Shanghai不然时间会少八小时 * * @return 完整的建表语句 */ public static String buildMySQLTargetTableDDL(MySQLConfig config) &#123; return String.format( MYSQL_TARGET_TABLE_TEMPLATE, "target_mysql_" + config.getTableName(), config.getTableSchema(), config.getHostname(), config.getPort(), config.getDbName(), config.getUsername(), config.getPassword(), config.getTableName() ); &#125; /** * desc：构建ElasticSearch目标表DDL * * @return 完整的建表语句 */ public static String buildEsTargetTableDDL(ElasticSearchConfig config) &#123; return String.format( ES_TARGET_TABLE_TEMPLATE, "target_es_" + config.getTableName(), config.getTableSchema(), config.getHosts(), config.getIndex() ); &#125; /** * desc：构建Kafka目标表DDL * * @return 完整的建表语句 */ public static String buildKafkaTargetTableDDL(KafkaConfig config) &#123; return String.format( KAFKA_TARGET_TABLE_TEMPLATE, "target_kafka_" + config.getTableName(), config.getTableSchema(), config.getFormat(), config.getTopic(), config.getBroker() ); &#125;&#125; InitHiveTables.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class InitHiveTables &#123; // 配置文件（可从配置文件或配置中心获取） private static final Map&lt;String, Object&gt; CONF; static &#123; // 资源MySQL数据库配置 Map&lt;String, String&gt; sourceTableMap = new HashMap&lt;&gt;(); sourceTableMap.put("t_regulation_matter", "id INT, matter_code STRING, matter_name STRING, PRIMARY KEY (id) NOT ENFORCED"); sourceTableMap.put("t_regulation_action", "id INT, action_name STRING, matter_code STRING, regulation_object_num STRING, regulation_object_name STRING, PRIMARY KEY (id) NOT ENFORCED"); MySQLConfig sourceDBConf = MySQLConfig.builder() .hostname("127.0.0.1").port("3306").username("root") .password("123456").dbName("flink-test").tableInfos(sourceTableMap).build(); // 目标MySQL数据库配置 MySQLConfig targetDBConf = MySQLConfig.builder() .hostname("127.0.0.1").port("3307").username("root") .password("123456").dbName("flink-test").tableName("t_regulation_matter") .tableSchema("id INT, matter_code STRING, matter_name STRING, sync_time TIMESTAMP, PRIMARY KEY (id) NOT ENFORCED").build(); // 目标Kafka配置 KafkaConfig kafkaConf = KafkaConfig.builder() .broker("127.0.0.1:9092").topic("t_regulation_matter-change_log").format("debezium-json") .tableName("t_regulation_matter").tableSchema("id INT, matter_code STRING, matter_name STRING, PRIMARY KEY (id) NOT ENFORCED").build(); // 目标ElasticSearch配置 ElasticSearchConfig elasticSearchConf = ElasticSearchConfig.builder() .hosts("http://localhost:9200").index("flink-matter_behavior_rel").tableName("regulation_matter_regulation_action_rel") .tableSchema("action_id INT, action_name STRING, matter_code STRING, matter_name STRING, regulation_object_num STRING, regulation_object_name STRING, PRIMARY KEY (action_id) NOT ENFORCED").build(); Map&lt;String, Object&gt; conf = new HashMap&lt;&gt;(); conf.put("sourceDBConf", sourceDBConf); conf.put("targetDBConf", targetDBConf); conf.put("kafkaConf", kafkaConf); conf.put("elasticSearchConf", elasticSearchConf); CONF = conf; &#125; public static void main(String[] args) &#123; // 获取执行环境 TableEnvironment tableEnvironment = EnvironmentUtils.getStreamTableEnvironment(); // 创建MySQL源表 MySQLConfig sourceDBConf = (MySQLConfig) CONF.get("sourceDBConf"); for (String sql : TableBuildUtils.buildBatchMySQLSourceTableDDL(sourceDBConf)) &#123; tableEnvironment.executeSql(sql).print(); &#125; // 创建MySQL目标表 MySQLConfig targetDBConf = (MySQLConfig) CONF.get("targetDBConf"); tableEnvironment.executeSql( TableBuildUtils.buildMySQLTargetTableDDL(targetDBConf) ).print(); // 创建Kafka目标表 KafkaConfig kafkaConf = (KafkaConfig) CONF.get("kafkaConf"); tableEnvironment.executeSql( TableBuildUtils.buildKafkaTargetTableDDL(kafkaConf) ).print(); // 创建ElasticSearch目标表 ElasticSearchConfig elasticSearchConf = (ElasticSearchConfig) CONF.get("elasticSearchConf"); tableEnvironment.executeSql( TableBuildUtils.buildEsTargetTableDDL(elasticSearchConf) ).print(); &#125;&#125; 六、提交初始化Hive表任务 执行命令 12345# 任务提交格式$&#123;flink安装路径&#125;/flink-1.13.6/bin/flink run -m $&#123;flink地址&#125; -c $&#123;类的全路径&#125; $&#123;服务jar包所在路径&#125;# 案例（进入安装目录）bin/flink run -m 127.0.0.1:8081 -c io.cheery.flink.example.InitHiveTables ./app-jar/Flink-SQL-CDC-0.0.1-SNAPSHOT-jar-with-dependencies.jar 进入Hive查看表 12345678hive&gt; show tables;OKsource_mysql_t_regulation_actionsource_mysql_t_regulation_mattertarget_es_regulation_matter_regulation_action_reltarget_kafka_t_regulation_mattertarget_mysql_t_regulation_matterTime taken: 0.134 seconds, Fetched: 5 row(s)]]></content>
      <categories>
        <category>ETL</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】Hive-v2.3.4安装]]></title>
    <url>%2Fposts%2F42b1cdd%2F</url>
    <content type="text"><![CDATA[下载地址 http://archive.apache.org/dist/hive/hive-2.3.4/ 配置环境变量 修改.bash_profile文件，添加如下配置1234567891011121314151617181920# 编辑文件vim ~/.bash_profile# 添加配置export HIVE_HOME=$&#123;安装路径&#125;/apache-hive-2.3.4export PATH=$PATH:$HIVE_HOME/bin# 配置生效source ~/.bash_profile# 检验是否配置好了hive --version# 出现如下结果表示成功--Hive 2.3.4Git git://daijymacpro-2.local/Users/daijy/commit/hive -r 56acdd2120b9ce6790185c679223b8b5e884aaf2Compiled by daijy on Wed Oct 31 14:20:50 PDT 2018From source with checksum 9f2d17b212f3a05297ac7dd40b65bab0-- 修改配置文件 修改并重命名/apache-hive-2.3.4/conf/hive-env.sh.template为 hive-env.sh 添加如下配置12345# HADOOP_HOME路径export HADOOP_HOME=$&#123;Hadoop安装路径&#125;# HIVE_CONF路径export HIVE_CONF_DIR=$&#123;Hive安装路径&#125;/apache-hive-2.3.4/conf /apache-hive-2.3.4/conf在目录下创建hive-site.xml文件并添加如下配置 Hive安装路径下创建warehouse目录123456789101112131415161718192021222324252627282930313233343536373839404142&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- warehouse路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;$&#123;安装路径&#125;/apache-hive-2.3.4/warehouse&lt;/value&gt; &lt;/property&gt; &lt;!-- 元数据存储路径 --&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://localhost:9083&lt;/value&gt; &lt;/property&gt; &lt;!-- MySQL连接地址 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;$&#123;JDBC连接地址&#125;/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;!-- MySQL账号密码 --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;$&#123;MySQL账号&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;$&#123;MySQL密码&#125;&lt;/value&gt; &lt;/property&gt; &lt;!-- MySQL驱动（6.x以上用com.mysql.cj.jdbc.Driver） --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver or com.mysql.cj.jdbc.Driver&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; /apache-hive-2.3.4/lib下添加MySQL驱动 启动命令 初始化MySQL Hive数据库 1234567891011# 进入/apache-hive-2.3.4/bin目录执行如下命令./schematool -initSchema -dbType mysql# 出现如下结果表示成功---Metastore connection URL: $&#123;JDBC连接地址&#125;/hive?createDatabaseIfNotExist=trueMetastore Connection Driver : com.mysql.jdbc.DriverMetastore connection User: root...schemaTool completed--- 进入MySQL中查看看到hive库的数据表 启动metastore 123456789# 进入/apache-hive-2.3.4/bin目录执行如下命令./hive --service metastore &gt;/dev/null 2&gt;&amp;1 &amp;# 查看9083端口有被使用就成功了lsof -i:9083---COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 46669 xxx 514u IPv4 0x45e025727f0f8129 0t0 TCP *:9083 (LISTEN)--- 启动Hive 在启动之前必须保证Hadoop（HDFS和YARN）已启动12345678# 在任意目录或/apache-hive-2.3.4/bin目录输入如下命令hive 或者 ./hive# 执行查看数据库的命令（出现如下结果表示正常）hive&gt; show databases;OKdefaultTime taken: 0.34 seconds, Fetched: 1 row(s)]]></content>
      <categories>
        <category>软件安装</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】Hadoop-v2.10.1安装]]></title>
    <url>%2Fposts%2F3de88dc%2F</url>
    <content type="text"><![CDATA[下载地址 https://downloads.apache.org/hadoop/common/hadoop-2.10.1/ 配置环境变量 修改.bash_profile文件，添加如下配置12345678910111213141516171819202122# 编辑文件vim ~/.bash_profile# 添加配置export HADOOP_HOME=$&#123;安装路径&#125;/hadoop-2.10.1export PATH=$PATH:$HADOOP_HOME/bin# 配置生效source ~/.bash_profile# 检验是否配置好了hadoop version# 出现如下结果表示成功--Hadoop 2.10.1Subversion https://github.com/apache/hadoop -r 1827467c9a56f133025f28557bfc2c562d78e816Compiled by centos on 2020-09-14T13:17ZCompiled with protoc 2.5.0From source with checksum 3114edef868f1f3824e7d0f68be03650This command was run using $&#123;安装路径&#125;/hadoop-2.10.1/share/hadoop/common/hadoop-common-2.10.1.jar-- 修改配置文件 修改hadoop-2.10.1/etc/hadoop/hadoop-env.sh（添加Java路径和Hadoop路径） 12export JAVA_HOME=$&#123;安装路径&#125;/Homeexport HADOOP_CONF_DIR=$&#123;安装路径&#125;/hadoop-2.10.1/etc/hadoop 修改 hadoop-2.10.1/etc/hadoop/core-site.xml （添加文件系统路径和文件存放路径） 123456789101112&lt;configuration&gt; &lt;!-- 配置hdfs文件系统路径 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:8082&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置hadoop运行时产生文件的存放路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;$&#123;宿主机任意目录&#125;&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hadoop-2.10.1/etc/hadoop/hdfs-site.xml 123456789101112&lt;configuration&gt; &lt;!-- hdfs保存数据副本的数量（伪分布式模式此值必须为1，默认值为3） --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭写入权限 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改 hadoop-2.10.1/etc/hadoop/yarn-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改并重命名hadoop-2.10.1/etc/hadoop/mapred-site.xml.template为 mapred-site.xml 1234567&lt;configuration&gt; &lt;!-- 指定mapreduce运行在yarn上 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动命令 启动NameNode hadoop namenode -format（出现以下字样表示启动成功）123/************************************************************SHUTDOWN_MSG: Shutting down NameNode at xxx************************************************************/ 启动HDFS cd ${文件路径}/hadoop-2.10.1/sbin &amp;&amp; ./start-dfs.sh 能访问 http://localhost:50070/explorer.html#/ 表示成功 启动yarn cd ${文件路径}/hadoop-2.10.1/sbin &amp;&amp; ./start-yarn.sh 中断输入 jps 输出以下内容表示成功 12345678917440 TaskManagerRunner12945 ResourceManager91154 NodeManager90722 SecondaryNameNode90278 NameNode90457 DataNode97579 TaskManagerRunner17194 StandaloneSessionClusterEntrypoint68620 Jps]]></content>
      <categories>
        <category>软件安装</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【ETL】Flink-v1.13.6安装]]></title>
    <url>%2Fposts%2Fe00738f%2F</url>
    <content type="text"><![CDATA[下载地址 https://downloads.apache.org/flink/flink-1.13.6/ 常用文件&amp;文件夹 bin/start-cluster.sh =&gt; 启动flink bin/stop-cluster.sh =&gt; 关闭flink bin/sql-client.sh =&gt; SQL命令行 conf/flink-conf.yaml =&gt; 核心配置文件 conf/sql-client-defaults.yaml =&gt; SQL命令配置文件 lib =&gt; 存放flink运行时的依赖 Web页面和CLI命令行 http://127.0.0.1:8081/#/overview ./sql-client.sh]]></content>
      <categories>
        <category>软件安装</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【软考中级】信息安全]]></title>
    <url>%2Fposts%2Fc3141c4%2F</url>
    <content type="text"><![CDATA[一、对称型加密 使用同一套密钥 Ke = Kd 加密强度不高，效率高 常见的加密算法：DES、3DES(3重DES)、RC-5、IDEA 二、非对称型加密 密钥必须成对使用(公钥加密、私钥解密)，Ke ≠ Kd 强度高，效率不高 常见的加密算法：RSA、ECC 三、数字签名 只有信息的发送者才能产生的别人无法伪造的一段数字串，这段数字串同时也是对信息的发送者发送信息真实性的一个有效证明 http://www.ruanyifeng.com/blog/2011/08/what_is_a_digital_signature.html 四、消息摘要 由单向散列函数加密成固定长度的加密值 摘要算法是不可逆的 常见的摘要算法：SHA、MD5 SHA和MD5的散列值分别为160和128位，所以SHA相对于比较安全 五、PKI公钥体系 管理密钥的注册、更新、销毁 六、各个网络层次的安全保障 物理层的安全保障一般是通过物理隔离的方式，比如用铺设专线 从第二层开始就是基本上都是走的协议来保证安全，所谓协议，就是通信的时候封包的一种规则 SSL，它是跨越了传输层一直延伸到应用层，所以遇到SSL的时候就要注意了，不建议你直接给SSL做一个定论，说他是传输层还是应用层，应该先把SSL抛开，看看其他的备选项是属于那一个层次，然后再来依照题意来分析SSL应该属于那一层。 七、网络威胁与攻击 威胁名称 描述 重放攻击（ARP） 所截获的某次合法通信数据拷贝，出于非法的目的而被重新发送 拒绝服务（DOS） 对信息或其他合法资源访问被无条件阻止 窃听 用各种可能的合法或非法的手段窃取系统中的信息资源和敏感信息。例如对通信线路中传输的信号进行搭线监听，或者利用通信设备在工作过程中产生的电磁泄露截取有用信息等。 业务流分析 通过对系统进行长期监听，利用统计分析方法对诸如通信频度、通信的信息流向、通信总量的变化等参数进行研究，从而发现有价值的信息和规律。 信息泄漏 信息被泄露或透露给某个非授权的实体 破坏信息的完整性 数据被非授权地进行增删、修改或破坏而受到损失 非授权访问 某一资源被某个非授权的人、或以非授权的方式使用 假冒 通过欺骗通信系统（或用户）达到非法用户冒充合法用户，或者特权小的用户冒充冲特权大的用户的目的，黑客大多是采用假冒进行攻击。 旁路控制 攻击者利用系统的安全缺陷或安全性伤的脆弱之处获得非授权的权利或特权。例如，攻击者通过各种攻击手段发现原本应保密，但是却又暴露出的一些系统“特性”。利用这些“特性”，攻击者可以绕过防线侵入系统的内部 授权侵犯 被授权以某一目的的使用某一系统或资源的某个人，却将次全县用于其它非授权的目的，也称作“内部攻击” 特洛伊木马 软件中含有一个察觉不出的或者无害的程序段，当它被执行时，会破坏用户的安全 陷阱门 在某个系统或者某个部件中设置了“机关”，使得当提供特定的输入数据时允许违反安全策略 抵赖 这是一种来自用户的攻击，比如：否认自己曾经发布过的某条信息，伪造一份对方的来信等]]></content>
      <categories>
        <category>软考笔记</category>
      </categories>
      <tags>
        <tag>软考软件设计师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【软考中级】计算机网络]]></title>
    <url>%2Fposts%2Fc3141c2%2F</url>
    <content type="text"><![CDATA[一、OSI七层模型 层次 名称 功能 主要设备 协议 7 应用层 为应用程序提供服务 / POP3、FTP、HTTP、Telnet、SMTP、DHCP、TFTP、SNMP、DNS 6 表示层 数据的格式与表达、加密、压缩 5 会话层 建立、管理和终止会话 4 传输层 端到端的连接 / TCP、UDP 3 网络层 实现两个端系统之间的数据透明传送，使传输层不需要了解网络中的数据传输和交换技术。这一层开始涉及到IP 三层交换机、路由器 ARP、RARP、IP、ICMP、IGMP 2 数据链路层 传送以帧位单位的信息 网桥、交换机、网卡 PPTP、L2TP、SLIP、PPP 1 物理层 规范传输介质的规格特性（接口大小、形状、引线数量、电压范围等），让比特流能在各种传输媒体之间传输。 中继器、集线器 / 二、TCP/IP协议族 TCP/IP协议称为可靠的连接，因为TCP在建立连接时会进行三次握手 UDP直接将数据包从源地址发到目标地址，而没有验证反馈的过程，所以UDP的性能较好，但是它的可靠性较差 DNS（Domain Name System）协议，将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网 递归查询：服务器必需回答目标IP与域名的映射关系 迭代查询：服务器收到一次迭代查询回复一次结果，这个结果不一定是目标IP与域名的映射关系，也可以是其他DNS服务器地址 TCP/IP模型 OSI七层模型 协议 网络接口层 数据链路层 CSMA/CD、TokingRing 网络接口层 物理层 CSMA/CD、TokingRing 网际层 网络层 ARP、RARP、IP、ICMP、IGMP 传输层 传输层 TCP、UDP 应用层 应用层 网络文件系统=&gt;NFS 应用层 表示层 基于TCP=&gt; Telent(23)、SMTP(25)、FTP(20/21)、HTTP(80)、HTTPS(443)、 POP3(110) 应用层 会话层 基于UDP=&gt; SNMP(161)、DNS(53)、DHCP(67)、TFTP(69) 三、IP地址 A类 取值范围为0～127 前8位为网络号，后24位为主机号，所以分配给主机的IP数量是$2^{24} - 2$ $2^{24} - 2$：IPv4规定了地址一共有32个bit位，而A类地址规定了前8位是网络号，后面的24个bit位都是主机号。但二进制全0和全1的IP地址有特殊含义，不能分配出去，所以能分配给主机的IP数量就是$2^{24} - 2$个 B类 取值范围为128～191 前16位为网络号，后16位为主机号，所以分配给主机的IP数量是$2^{16} - 2$ C类 取值范围为192～223 前124位为网络号，后8位为主机号，所以分配给主机的IP数量是$2^{8} - 2$ 四、子网划分 子网掩码 将一个网络划分为多个子网(取部分主机号当子网号) 将多个网络合并成一个大的网络(取部分网络号当主机号) 考试中常见题型 1、将B类IP地址168.195.0.0划分成27个子网，子网掩码是多少？ 解题思路 由于题干中已经告诉我们是B类IP，所以前16位是网络号 其次将IP转为转为二进制 10101000 11000011 00000000 00000000 现在我们要划分为27个子网(其实就是从主机号借N位当子网号)，取1个bit位，可以得到2个子网，取2个bit位可以得到4个子网，取3个bit位可以得到8个子网……以此类推，所以27个子网需要取5个bit位来做子网号 得到子网掩码为 11111111 11111111 11111000 00000000，将其转为十进制255.255.248.0 8个1代表255 11111000后三位为0，所以可以转换为2^2+2^1+2^0=7，255-7=248 2、将B类IP地址168.195.0.0划分成若干子网，每个子网内有主机700台，子网掩码为多少？ 解题思路 由于题干中已经告诉我们是B类IP，所以前16位是网络号 已知子网内有主机700台，所以可以套用如下公示2^N - 2 &gt;= 主机数 2^N - 2 &gt;= 700，得出N=10 所以主机号需要占10位，剩下的6位可以借给当网络号 得到子网掩码为 11111111 11111111 11111100 00000000，将其转为十进制255.255.252.0 8个1代表255 11111100后二位为0，所以可以转换为2^1+2^0=3，255-3=252五、网络规划与设计]]></content>
      <categories>
        <category>软考笔记</category>
      </categories>
      <tags>
        <tag>软考软件设计师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【软考中级】数据库系统]]></title>
    <url>%2Fposts%2Fc3141c1%2F</url>
    <content type="text"><![CDATA[一、三级模式、两级映射 二、E-R模型 椭圆形代表属性 矩形代表实体 菱形代表关系 特殊的矩形代表实体的特殊化 三、关系代数 关系代数基本的运算有： 并：取两个表全部的数据 交：取两个表相交的数据 差：取两个表相差的数据 笛卡尔积：类似于乘法运算，比如有S1和S2两个集合，将S1集合的每一条记录都S2的每一条记录都做一次组合，所得结果就是笛卡尔积 投影：选列的一种操作，比如一个学生表中有姓名，班级，学号等字段。我们只查其中的姓名和学号，这就是投影 选择：SQL语句中WHERE后面的筛选条件 联接：INNER JOIN / LEFT JOIN / RIGHT JOIN 四、规范化理论4.0、求候选键 将关系的函数依赖关系，用“有向图”的方式表示 找出入度为0的属性，并以该属性集合为起点，尝试遍历有向图，若能正常遍历图中所有结点,则该属性集即为关系模式的候选键 若入度为0的属性集不能遍历图中所有结点，则需要尝试性的将一些中间结点(既有入度，也有出度的结点)并入入度为0的属性集中,直至该集合能遍历所有结点，集合为候选键 案例 4.1、主属性与非主属性 组成候选码的属性就是主属性，其它的就是非主属性 例: 关系模式CSZ(CITY，ST，ZIP)，其属性组上的函数依赖集为: F= { (CITY，ST)→ZIP，ZIP→CITY } { ST CITY } 可以推导出ZIP { ST ZIP } 可以推导出CITY 4.2、范式 第一范式(1NF) 在关系模式R中，当且仅当所有域只包含原子值，即每个属性都是不可再分的数据项，则称关系模式R是第一范式 该例题中将高级职称人数删除即可，才能保证原子性 第二范式(2NF) 当且仅当关系模式R是第一范式( 1NF) ，且每一个非主属性完全依赖候选键(没有不完全依赖)时，则称关系模式R是第二范式 该例题所述问题分为两个表即可 Tab1=&gt;学生表{ 学号，课程号，成绩 } Tab2=&gt;学分表{ 课程号，学分 } 第三范式(3NF): 当且仅当关系模式R是第二范式( 2NF) ，且R中没有非主属性传递依赖于候选键时，则称关系模式R是第三范式。 该例题所述问题分为两个表即可 Tab1=&gt;学生表{ 学号，姓名，系号 } Tab2=&gt;系表{ 系号，系名，系位置 } 五、SQL语句 DDL（Data Definition Languages） 数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。 DML（Data Manipulation Language） 数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查） DCL（Data Control Language） 数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。 并发控制 数据库完整性约束 实体完整性约束 主键 ==&gt; 非空 &amp; 唯一 参照完整性约束 外键 ==&gt; 可为空 || 正确的属性 用户自定义完整性约束 对数据库字段做约束要求 六、数据备份 / 概念 优点 缺点 冷备份 冷备份也被称为静态备份，是将数据库正常关闭，在停止状态下，将数据库的文件全部备份（复制）下来 非常快速的备份方法（只需复制文件）；容易归档（简单复制即可）；容易恢复到某个时间点上（只需将文件再复制回去）；能与归档方法相结合，做数据库“最佳状态”的恢复；低度维护，高度安全 单独使用时，只能提供到某一时间点伤的恢复；在实施备份的全过程中，数据库必须要做备份而不能做其他工作；若磁盘空间有限只能复制到磁带等其他外部存储设备上，速度会很慢；不能按表或按用户恢复 热备份 热备份也称为动态备份，是利用备份软件，在数据库正常运行的状态下，将数据库中的数据文件备份出来 可在表空间或数据库文件级备份，备份的时间短；备份时数据库仍可使用；可达到妙级恢复（恢复到某一时间点上）；可对几乎所有数据库实体做恢复 不能出错，否则后果严重；若热备份不成功，所得结果不可用于时间点的恢复；因难与维护，所以要特别小心，不允许“已失败告终”]]></content>
      <categories>
        <category>软考笔记</category>
      </categories>
      <tags>
        <tag>软考软件设计师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【软考中级】计算机组成与结构]]></title>
    <url>%2Fposts%2Fc3141cd%2F</url>
    <content type="text"><![CDATA[一、数据的表示 R进制转十进制【按权展开法】 概念 R进制转十进制使用按权展开法，其具体操作方式为:将R^k进制数的每位数值用R形式表示，即幂的底数是R，指数为k，k与该位和小数点之间的距离有关。当该位位于小数点左边，k值是该位和小数点之间数码的个数，而当该位位于小数点右边，k值是负值，其绝对值是该位和小数点之间数码的个数加1。 案例 二进制4792.053 4x2^3 + 7x2^2 + 9x2^1 + 2x2^0 + 0x2^-1 + 5x2^-2 + 3x2^-3 十进制转R进制【短除法】 二进制转八进制【取三合一法】 概念 从二进制的小数点为分界点，向左（向右）每三位取成一位，接着将这三位二进制按权相加，得到的数就是一位八位二进制数，然后，按顺序进行排列，小数点的位置不变，得到的数字就是我们所求的八进制数。如果向左（向右）取三位后，取到最高（最低）位时候，如果无法凑足三位，可以在小数点最左边（最右边），即整数的最高位（最低位）添0，凑足三位。 案例 二进制101110.101 从右到左每三位取成一位 101 / 110 / 101 101 =&gt; 1x2^2 + 0x2^1 + 1x2^0 =&gt; 5 110 =&gt; 1x2^2 + 1x2^1 + 0x2^0 =&gt; 6 101 =&gt; 1x2^2 + 0x2^1 + 1x2^0 =&gt; 5 小数点不变得出的结果是 =&gt; 56.5 八进制转二进制【除2取余法】 概念 八进制数通过除2取余法，得到二进制数，对每个八进制为3个二进制，不足时在最左边补零。 案例 八进制数326 6 =&gt; 110 2 =&gt; 010 3 =&gt; 011 结果为 =&gt; 11010110 二进制转十六进制【取四合一法】【8421法】 概念 二进制转换成十六进制的方法是，取四合一法，即从二进制的小数点为分界点，向左（或向右）每四位取成一组。组分好以后，对照二进制与十六进制数的对应关系，将四位二进制按权相加，得到的数就是一位十六进制数，然后按顺序排列，小数点的位置不变，最后得到的就是十六进制数。 案例 二进制10101101110 从右到左每四位取成一位 1110 / 0110 / 0101 1110 =&gt; 0x1 + 1x2 + 1x4 + 1x8 = 14 =&gt; E 0110 =&gt; 0x1 + 1x2 + 1x4 +0x8 = 6 0101 =&gt; 1x1 + 0x2 + 1x4 + 0x8 = 5 结果为 =&gt; 56E 十六进制转二进制【一分为四法】【8421法】 十六进制基于0～9 A～F的基数 概念 每一位十六进制数，转换为四位二进制数。 案例 十六进制数8 8 / 4 / 2 / 1 1 / 0 / 0 / 0 十六进制数E（14） 8 / 4 / 2 / 1 1 / 1 / 1 / 0 十六进制数E8 将8和E分别拆开组合 11101000 二进制、八进制、十六进制之间的关系 二、原码、反码、补码、移码 概念 一个数在计算机中的二进制表示形式, 叫做这个数的机器数。码值用8位表示，高位表示符号(0为正数1为负数)。 正数的原码、反码、补码一致，移码基于补码将符号位取反。 负数的反码与原码后7位相反，补码等于反码加一，移码基于补码将符号位取反。 案例| 数/码 | 数值1 | 数值-1 | 数值1+(-1) || :—: | :—: | :—: | :—: || 原码 | 00000001 | 10000001 | 10000010 || 反码 | 00000001 | 11111110 | 11111111 || 补码 | 00000001 | 11111111 | 00000000 || 移码 | 10000001 | 01111111 | 10000000 | 三、 数值表示范围和浮点的运算 数据的表示范围 浮点的运算 四、计算机结构 主机 主储存器 运算器【CPU的重要组成部分】 算数逻辑单元ALU 数据的算术运算和逻辑运算 累加寄存器AC 通用寄存器，为ALU提供一个工作区 ，用在暂存数据 数据缓冲寄存器DR 写内存时，暂存指令或数据 状态条件寄存器（特殊情况下可归为控制器） 存状态标志与控制标志 控制器【CPU的重要组成部分】 程序计数器PC 存储下一条要执行指令的地址 指令寄存器IR 存储即将执行的指令 地址寄存器AR 用来保存当前CPU所访问的内存单元的地址 指令编译器IR 对指令中的操作码字段进行分析解释 时序部件 提供时序控制信号 外设 辅助储存器 输入输出设备 五、 计算机体系结构分类 Flynn分类法 英文全称 单个 =&gt; Single 多个 =&gt; Multiple 指令 =&gt; Instructions 数据 =&gt; Data 流 =&gt; Stream 指令的基本概念 一条指令就是机器语言的一个语句，它是一组有意义的二进制代码，指令的基本格式如下: 操作码字段 指出计算机要执行什么样的操作，比如: 加减乘除法 地址码字段 包含各操作数的地址及操作结果的存放地址 从地址结构的角度可以分为 三地址指令 OP / A1 / A2 / A3 操作 / 操作值1 / 操作值2 / 操作值3 案例 a+b=c 加法代表指令，abc分别代表值 二地址指令 OP / A1 / A2 操作 / 操作值1 / 操作值2 案例 a+=b 加法代表指令，ab分别代表值 一地址指令 OP / A1 操作 / 操作值1 零地址指令 OP 操作 寻指方式 复杂指令集【CISC】和精简指令集【RISC】 【CISC】 复杂 / 指令数量多 / 频率差别大 / 多寻址 【RISC】 精简 / 指令数量少 / 操作寄存器 / 单周期 / 少寻址 / 多通用寄存器 / 支持流水线 六、 流水线 概念 流水线是指在程序执行时多条指令重叠进行操作的一种准并行处理实现技术。即可以同时为多条指令的不同部分进行工作，以提高各部件的利用率和指令的平均执行速度。 执行一条指令的过程中，最少要经历（取指 -&gt; 分析 -&gt; 执行）三个步骤。 流水线的计算 流水线的执行时长 流水线周期（△t）为指令执行阶段中执行时间最长的一段。 流水线计算公式 一条指令的执行时间 + (指令条数 - 1) * 流水线周期 理论公式 (t1 + t2 + …tk) + (n - 1) * △t 实践公式 (k + n - 1) * △t k为一条指令所包含的部分的多少 案例 一条指令的执行过程可以分解为取指、分析和执行三步，在取指时间t取指=3△t、分析时间t分析=2△t、执行时间t执行=4△t的情况下，若按串行方式执行，则10条指令全部执行完需要( )△t;若按流水线的方式执行，流水线周期为( ) △t ,则10条指令全部执行完需要( ) △t。 第一空 ==&gt; (3 + 2 + 4) x 10 = 90△t 第二空 ==&gt; 4△t 第三空 ==&gt; 理论公式得出的结果 ==&gt; (3 + 2 + 4) + (10 -1) x 4 = 45△t 实际公式得出的结果 ==&gt; (3 + 10 - 1) x 4△t = 48△t 流水线的加速比计算 概念 流水线的加速比指的是在完成同一批任务时，不使用流水线所用的时间与使用流水线所用的时间之比称之为流水下的加速比。加速比是越大越好的，它呈现了使用流水线的效果的好坏程度。 计算公式 S = 不使用流水线执行时间 / 使用流水线的执行时间 案例（题目同上） 得出的结果 ==&gt; S = 90 / 45 = 2 流水线的吞吐率 概念 流水线的吞吐率指的是在单位时间内流水线所完成的任务数量或输出的结果数量。 计算公式 基本公式 TP = 指令条数 / 流水线执行时间 最大吞吐率公式 TP(max) = 1 / △t 案例（题目同上） TP ==&gt; 10 / 45 = 2 / 9 TP(max) ==&gt; 1 / 4 七、 层次化储存结构 结构概念图 Cache概念 Cache（高速缓存）处于CPU和主存之间，它是为了提高访问的速度而提出来的一种设计方案。 在计算机的存储系统体系中，Cache是访问速度最快的层次(若有寄存器，则寄存器最快)。 使用Cache改善系统性能的依据是程序的局部性原理。 CPU要获取数据的流程是先从Cache中找，如果找不到的话，就去内存中找。而从Cache中找到的概率叫做访问命中率，比如说100次访问Cache，有95次能找到要的数据，那么访问命中率就是95%。 案例 我们假设Cache的访问命中率是95%，Cache的存取周期是1ns，主存的存期周期是1ms，求平均周期。 1ms = 1000ns 95% x 1 + (1 - 95%) x 1000 = 50.95ns Cache映像 直接相连映像：硬件电路比较简单，但是冲突率高。 全相联映像：电路难于设计和实现，只适用于小容量的cache，冲突率较低，空间利用率高 。 组相联映像：直接相联与全相联的折中。 八、 主存-编址与计算 九、 总线 一条总线统一时刻仅允许一个设备发送，但是允许多个设备接收。 总线三大分类: 数据总线(Data Bus):在CPU与RAM之间来回传送需要处理或是需要储存的数据。 地址总线(Address Bus):用来指定在RAM ( Random Access Memory)之中储存的数据的地址。 控制总线(Control Bus): 将微处理器控制单元( Control Unit )的信号,传送到周边设备，一般常见的为USB Bus和1394 Bus。 十、 串联系统与并联系统可靠度计算 串联系统 所有子系统都必须正常运行，整个系统才能正常，只要有一个环节出问题了，整个系统就不能正常运行。 可靠度 将所有子系统的可靠度累乘起来 失效率 各个环节失效率累加起来 并联系统 只要有一个子系统能够正常运行，那整个系统都能正常运行 可靠度 1 - 所有子系统都失效的概率 失效率 子系统的失效率累乘，就得到了串联模型的失效率 串联+并联系统 可靠度 根据实际情况拆分，再根据不同系统的可靠度公式进行计算]]></content>
      <categories>
        <category>软考笔记</category>
      </categories>
      <tags>
        <tag>软考软件设计师</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序消息推送开发调用流程]]></title>
    <url>%2Fposts%2F663210d%2F</url>
    <content type="text"><![CDATA[一、官方文档https://developers.weixin.qq.com/miniprogram/dev/framework/open-ability/subscribe-message.html 二、使用 前置准备 登录微信公众平台配置模版【https://mp.weixin.qq.com】 点击添加可以从模板库中选择或者创建等待微信审批 该页面的进入需要翻到最后一页 从公共模板库中添加一个模板 回到列表页后就能看到刚申请到的模板信息【模板ID在之后的接口需要用到】 点击详情可以看到模板参数名【在之后的接口需要用到】 前端 需要调用该API引导用户订阅消息 wx.requestSubscribeMessage 伪代码如下 123456789101112131415161718// 必须使用点击事件触发该API原因是需要用户授权wx.showModal(&#123; title: '提示', content: '是否开启订阅', success: function(res) &#123; if (res.confirm) &#123; wx.requestSubscribeMessage(&#123; tmplIds: [''], // 需要订阅的消息模板的ID的集合，一次调用最多可订阅3条消息！ success(res) &#123; console.log(res) &#125;, fail(res) &#123; console.log(res) &#125; &#125;); &#125; &#125;&#125;); 订阅成功返回 1234&#123; 模板ID: "accept", // [TEMPLATE_ID]是动态的键，即模板id，值包括'accept'、'reject'、'ban'。'accept'表示用户同意订阅该条id对应的模板消息，'reject'表示用户拒绝订阅该条id对应的模板消息，'ban'表示已被后台封禁。 errMsg: "requestSubscribeMessage:ok" // 接口调用成功时errMsg值为'requestSubscribeMessage:ok'&#125; 后端 需要调用该API实现消息推送 1https://api.weixin.qq.com/cgi-bin/message/subscribe/send?access_token=ACCESS_TOKEN 官方参数如下 伪代码参数如下 1234567891011121314151617181920212223&#123; "touser": "xxx", "template_id": "xxx", "miniprogram_state": "developer", "lang": "zh_CN", "data": &#123; "thing1": &#123; "value": "中共长沙市委常委会" &#125;, "date2": &#123; "value": "2020年11月20日" &#125;, "thing3": &#123; "value": "市委大楼" &#125;, "thing4": &#123; "value": "推进" &#125;, "thing5": &#123; "value": "请各位委员及时参加" &#125; &#125;&#125; PostMan请求示例 登录微信查看消息服务就能接收到该条消息 三、问题 消息推送接口中的【access_token】如何获取？ 官方文档：https://developers.weixin.qq.com/doc/offiaccount/Basic_Information/Get_access_token.html 消息推送接口中的【touser】如何获取？ 官方文档：https://developers.weixin.qq.com/miniprogram/dev/api-backend/open-api/login/auth.code2Session.html]]></content>
      <categories>
        <category>小程序</category>
      </categories>
      <tags>
        <tag>小程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录项目OpenAPI服务调用交互]]></title>
    <url>%2Fposts%2F92330f4%2F</url>
    <content type="text"><![CDATA[一、前置准备 第三方系统需要向党建项目组订阅需要调用的API 第三方系统需要向党建项目组提供各环境调用服务器IP 党建项目组通过后向各第三方系统分配clientId和clientSecret 二、全局响应码API每次调用可能获得正确或错误的返回码，调用方可根据返回码信息调试接口或给API提供方反馈错误 响应码 说明 0000 操作成功 0001 服务异常 0002 操作失败 0003 空指针 0004 系统异常 0005 请求方式不支持 0006 幂等拦截 1001 数据校验失败 1002 数据不完整 1003 非法数据（与接口所要求的数据不一致） 1004 数据未找到 8003 身份令牌不存在 8004 令牌错误 8005 身份令牌超时 8006 没有操作权限 8007 账号被锁定 9001 主键冲突 9002 重复键冲突 9003 数据过长 三、API共性说明 公共请求头 请求头名称 内容 Accept application/json Content-Type application/json;charset=utf-8 x-pb-client 党建提供的第三方客户端ID x-pb-nonce 32位随机值 x-pb-sign 签名 x-pb-timestamp 毫秒时间戳（与服务器时间不能相隔180s） 签名生成 获取请求的参数 Query方式 截取URL问号后面的参数 如https://www.api.com?name=foobar&amp;age=20 则截取name=foobar&amp;age=20即可 Body方式 获取请求体里面全部参数，进行ASCII码排序。0～9&lt;A～Z&lt;a～z。 如 {age=20, content=foobar, dataId=foobar, publisher=foobar} 去掉原有参数的引号 构建签名（先使用sha256进行加密再用MD5进行加密后转大写） Java 使用org.apache.commons.codec.digest包 123456789/** * clientSecret 客户端秘钥 * params 请求的参数 * nonce 随机值 * timestamp 时间戳 */private String generateSign(String clientSecret, Object params, String nonce, String timestamp) &#123; return DigestUtils.sha256Hex(clientSecret + params + nonce + timestamp).toUpperCase();&#125; 调用示例 PostMan发送GET请求 Pre-request Script 123456789101112131415161718192021// 时间戳var timestamp = (Date.now()).toFixed();// 客户端秘钥var clientSecret = 'your clientSecret';// 36位随机数var nonce = randomVal();// 签名值var sign = CryptoJS.SHA256(clientSecret + 'your parameters' + nonce + timestamp).toString(CryptoJS.enc.Hex).toUpperCase();// 设置到postman全局变量postman.setGlobalVariable("timestamp", timestamp)postman.setGlobalVariable("nonce", nonce)postman.setGlobalVariable("sign", sign)// 生成36位随机数function randomVal() &#123; return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) &#123; var r = Math.random()*16|0, v = c == 'x' ? r : (r&amp;0x3|0x8); return v.toString(16); &#125;);&#125; 响应结果 Java发送GET请求 1234567891011121314151617181920212223242526public void sendGETRequest() &#123; // 客户端ID String clientId = "your clientId"; // 客户端秘钥 String clientSecret = "your clientSecret"; // 时间戳 String timeStamp = String.valueOf(System.currentTimeMillis()); // 36位随机数 String nonce = UUID.randomUUID().toString(); // signq签名 String sign = DigestUtils.sha256Hex(clientSecret + parameters + nonce + timeStamp).toUpperCase(); // 请求参数 String parameters = "pageNum=1&amp;pageSize=20"; // 构建请求头 HttpHeaders headers = new HttpHeaders(); headers.set("Content-Type", "application/json;charset=utf-8"); headers.set("Accept", "application/json"); headers.add("x-pb-client", clientId); headers.add("x-pb-nonce", nonce); headers.add("x-pb-timestamp", timeStamp); headers.add("x-pb-sign", sign); ResponseEntity&lt;String&gt; responseEntity = restTemplate.exchange("api address", HttpMethod.GET, new HttpEntity&lt;&gt;(null, headers), String.class); System.out.println(responseEntity);&#125; Java发送POST请求 123456789101112131415161718192021222324252627282930public void sendPOSTRequest() &#123; // 客户端ID String clientId = "your clientId"; // 客户端秘钥 String clientSecret = "your clientSecret"; // 时间戳 String timeStamp = String.valueOf(System.currentTimeMillis()); // 36位随机数 String nonce = UUID.randomUUID().toString(); // signq签名 String sign = DigestUtils.sha256Hex(clientSecret + parameters + nonce + timeStamp).toUpperCase(); // 请求参数 SortedMap&lt;String, Object&gt; parameters = new TreeMap&lt;&gt;(); parameters.put("parameter1", 1); parameters.put("parameter2", 2); parameters.put("parameter3", 3); // 构建请求头 HttpHeaders headers = new HttpHeaders(); headers.set("Content-Type", "application/json;charset=utf-8"); headers.set("Accept", "application/json"); headers.add("x-pb-client", clientId); headers.add("x-pb-nonce", nonce); headers.add("x-pb-timestamp", timeStamp); headers.add("x-pb-sign", sign); ResponseEntity&lt;String&gt; responseEntity = restTemplate.exchange("api address", HttpMethod.POST, new HttpEntity&lt;&gt;(parameters, headers), String.class); System.out.println(responseEntity);&#125; 正常情况返回的JSON包 12345&#123; "resultCode": "0000", "resultMessage": "操作成功", "resultObj": null&#125; 出参说明 参数字段 说明 resultCode 响应码 resultMessage 响应信息 resultObj 响应对象 四、第三方系统与本系统跳转交互考虑双方系统安全前提，所有跳转交互应基于token。交互流程图及请求示例伪代码如下： 交互流程图 第三方系统前端（触发业务跳转） 12345678910111213141516171819202122$.ajax(&#123; url: "https://www.api.com/redirects", type: "POST", headers: &#123; 'Content-Type': 'application/json' &#125;, data: JSON.stringify(&#123; token: 'token', redirectUrl: 'https://target-address.com/index.html' &#125;), dataType: 'JSON', success: function(res) &#123; if (res.resultCode === '0000') &#123; window.location.href = res.resultObj; &#125; else &#123; alert(res); &#125; &#125;, error: function(err) &#123; alert(err); &#125;&#125;); 目标系统后端（执行逻辑处理） 123456789101112131415161718192021222324@PostMapping("/redirects")public String systemRedirect(@RequestBody RedirectDTO redirectDTO) &#123; try &#123; // 1、调用第三方系统校验token的接口 ResponseEntity tokenCheckResponse = restTemplate.getForObject("https://www.other.api.com/token-check", ResponseEntity.class); if (HttpStatus.OK.equals(tokenCheckResponse.getStatusCode())) &#123; // 2、调用第三方系统获取用户信息的接口 ResponseEntity getUserResponse = restTemplate.getForObject("https://www.other.api.com/users/infos", ResponseEntity.class); if (HttpStatus.OK.equals(getUserResponse.getStatusCode())) &#123; // 3、得到用户信息 Object body = getUserResponse.getBody(); // 4、查询本系统用户信息 UserInfo user = queryDB(body); // 5、构建Token String token = buildToken(user); // 6、返回给第三方系统前端，由第三方执行跳转 return "https://target-address.com?redirectCode=" + token + "&amp;redirectUrl=" + redirectDTO.getRedirectUrl() + "&amp;tips="; &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; throw new RuntimeException("跳转异常请重试");&#125; 目标系统前端（执行地址跳转） 123456789101112131415161718192021222324let token = getQueryVariable('redirectCode');let redirectUrl = getQueryVariable('redirectUrl');if (token !== undefined) &#123; $.ajax(&#123; url: "https://www.api.com/users/infos", type: "GET", headers: &#123; 'Authorization': token &#125;, success: function(res) &#123; if (res.resultCode === '0000') &#123; // 1、将用户信息存入本地储存 window.localStorage.setItem('user', res.resultObj); // 2、执行第三方系统要跳转的页面 window.location.href = redirectUrl; &#125; else &#123; alert(res); &#125; &#125;, error: function(err) &#123; alert(err); &#125; &#125;)&#125; 关于第三方提供的校验token或获取用户信息API（仅支持GET和POST请求） Token必须由请求头Authorization传递 校验tokenJSON包返回格式（需严格遵守程序会根据该固定格式解析） 1234&#123; "code": "0", // 成功必须定义为字符串的0 "data": true or false&#125; 获取用户信息JSON包返回格式（需严格遵守程序会根据该固定格式解析） 1234&#123; "code": "0", // 成功必须定义为字符串的0 "data": 身份证 or 手机号 // 其他情况下返回 null&#125; 调用示例 Pre-request Script 1234567891011121314151617181920212223// 时间戳var timestamp = (Date.now()).toFixed();// 客户端秘钥var clientSecret = "your clientSecret";// 36位随机数var nonce = randomVal();// 参数var parameters = '&#123;redirectUrl=https://www.target-address.com, token=your token&#125;';// sign签名var sign = CryptoJS.SHA256(clientSecret + parameters + nonce + timestamp).toString(CryptoJS.enc.Hex).toUpperCase();// 设置到postman全局变量postman.setGlobalVariable("timestamp", timestamp)postman.setGlobalVariable("nonce", nonce)postman.setGlobalVariable("sign", sign)// 生成36位随机数function randomVal() &#123; return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) &#123; var r = Math.random()*16|0, v = c == 'x' ? r : (r&amp;0x3|0x8); return v.toString(16); &#125;);&#125; 响应结果]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用RequestInterceptor实现Feign向下游服务传递Token]]></title>
    <url>%2Fposts%2Fe27ae85%2F</url>
    <content type="text"><![CDATA[实现123456789101112131415161718192021@Componentpublic class FeignTokenInterceptor implements RequestInterceptor &#123; private static final String HEADER_AUTH = "Authorization"; @Override public void apply(RequestTemplate requestTemplate) &#123; HttpServletRequest httpServletRequest = getHttpServletRequest(); if (null == httpServletRequest) &#123; return; &#125; requestTemplate.header(HEADER_AUTH, httpServletRequest.getHeader(HEADER_AUTH)); &#125; private HttpServletRequest getHttpServletRequest() &#123; try &#123; return ((ServletRequestAttributes) Objects.requireNonNull(RequestContextHolder.getRequestAttributes())).getRequest(); &#125; catch (Exception e) &#123; return null; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HttpMessageConverter转换器]]></title>
    <url>%2Fposts%2F00e5cee%2F</url>
    <content type="text"><![CDATA[一、数据序列化和反序列化&ensp; &ensp; 序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换成可取用格式（例如存成文件，存于缓冲，或经由网络中发送），以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。对于许多对象，像是使用大量引用的复杂对象，这种序列化重建的过程并不容易。面向对象中的对象序列化，并不概括之前原始对象所关系的函数。这种过程也称为对象编组（marshalling）。从一系列字节提取数据结构的反向操作，是反序列化（也称为解编组、deserialization、unmarshalling）。——维基百科 &ensp; &ensp; 简单的来理解，序列化是将对象转换成可存储或传输的二进制数据，反序列化将二进制数据还原成原始对象。在Spring Web中常使用两个数据接收和响应的注解@RequestBody和@ResponseBody，在数据的处理时就可以理解为一组数据的序列化和反序列化过程。 二、Spring中的数据转换流程&ensp; &ensp; Http请求和响应都是基于文本传输的，@RequestBody的作用是将这些文本映射为具体的对象，@ResponseBody的作用是将数据解析成JSON输出，整个过程就是利用HttpMessageConverter进行转换。&ensp; &ensp; 在Spring Web应用中WebMvcConfigurationSupport这个类的addDefaultHttpMessageConverters方法会在程序启动时加载一些默认的数据转换器，之后我们的请求或者响应程序会根据JavaType和MediaType找到具体的消息转换器去执行。执行流程如下： 三、Spring中常见的消息转换器 转换器 支持的JavaType 支持的MediaType StringHttpMessageConverter String text/plain, / ResourceHttpMessageConverter Resource / MappingJackson2HttpMessageConverter Object application/json, application/*+json ByteArrayHttpMessageConverter byte[] application/octet-stream, / AllEncompassingFormHttpMessageConverter Map application/x-www-form-urlencoded, multipart/form-data 四、分析HttpMessageConverter 先通过源码分析这个接口具有的功能 1234567891011public interface HttpMessageConverter&lt;T&gt; &#123; boolean canRead(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); boolean canWrite(Class&lt;?&gt; clazz, @Nullable MediaType mediaType); List&lt;MediaType&gt; getSupportedMediaTypes(); T read(Class&lt;? extends T&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException; void write(T t, @Nullable MediaType contentType, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException;&#125; canRead和canWrite来控制对数据的读写权限 getSupportMediaTypes来获取支持的媒体类型集合 read和write来决定数据的转换，其中最重要的两个参数HttpInputMessage和HttpOutputMessage，都提供了getBody的方法用来获取数据的流信息，这样我们就能对请求和响应的数据进行处理 五、重写HttpMessageConverter&ensp; &ensp; 虽然FastJson也定义了自己的消息转换器FastJsonHttpMessageConverter，但是从WebMvcConfigurationSupport类中得知Spring默认支持的JSON序列化工具为Jackson和Gson。那么我们也可以试着利用FastJson来实现该功能。 方式一：添加FastJsonHttpMessageConverter序列化规则 编写一个多数据类型的对象 12345678910111213public class FoobarVO &#123; private String string; private Integer integer; private List list; private Boolean bool; private Map map; private LocalDate date;&#125; 添加配置 123456789101112131415161718192021222324252627282930@Beanpublic HttpMessageConverters fastJsonHttpMessageConverters() &#123; FastJsonHttpMessageConverter fastJsonHttpMessageConverter = new FastJsonHttpMessageConverter(); FastJsonConfig fastJsonConfig = new FastJsonConfig(); // 设置字符集 fastJsonConfig.setCharset(StandardCharsets.UTF_8); // 设置序列化规则 fastJsonConfig.setSerializerFeatures( // 如果是数据类型【字符串】为null时则输出"" SerializerFeature.WriteNullStringAsEmpty, // 如果是数据类型【数值】为null时则输出0 SerializerFeature.WriteNullNumberAsZero, // 如果是数据类型【集合】为null时则输出[] SerializerFeature.WriteNullListAsEmpty, // 如果是数据类型【Map】填充null SerializerFeature.WriteMapNullValue, // 如果是数据类型【布尔】为null时则输出false SerializerFeature.WriteNullBooleanAsFalse, // 日期转换 SerializerFeature.WriteDateUseDateFormat ); // 设置fastJson配置 fastJsonHttpMessageConverter.setFastJsonConfig(fastJsonConfig); // 设置支持的媒体类型 fastJsonHttpMessageConverter.setSupportedMediaTypes(Collections.singletonList(MediaType.APPLICATION_JSON_UTF8)); return new HttpMessageConverters(fastJsonHttpMessageConverter);&#125; 重写WebMvcConfigurer类的extendMessageConverters方法（检查我们添加的FastJson配置是否加载了） 1234@Overridepublic void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; converters.forEach(System.out::println);&#125; 控制台输出（能看到FastJsonHttpMessageConverter被加载到第一项）123456789com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter@5fb7b227org.springframework.http.converter.ByteArrayHttpMessageConverter@12ce9299org.springframework.http.converter.StringHttpMessageConverter@12aa9370org.springframework.http.converter.ResourceHttpMessageConverter@2338eeeforg.springframework.http.converter.ResourceRegionHttpMessageConverter@41fc5625org.springframework.http.converter.xml.SourceHttpMessageConverter@27c05ed9org.springframework.http.converter.support.AllEncompassingFormHttpMessageConverter@61377842org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@51cbd1dorg.springframework.http.converter.xml.Jaxb2RootElementHttpMessageConverter@26f29ee5 验证结果（能明显的看出使用FastJson的配置生效了） 默认消息转换器输出（MappingJackson2HttpMessageConverter） 123456789101112&#123; "resultCode": "0000", "resultMessage": "操作成功", "resultObj": &#123; "string": null, "integer": null, "list": null, "bool": null, "map": null, "date": null &#125;&#125; 使用FastJson消息转换器方式输出 123456789101112&#123; "resultCode": "0000", "resultMessage": "操作成功", "resultObj": &#123; "bool": false, "date": null, "integer": 0, "list": [], "map": null, "string": "" &#125;&#125; 方式二：自定义消息转换器（需要像其他消息转换器一样重写AbstractHttpMessageConverter) 编写自定义消息转换器 123456789101112131415161718192021222324252627282930313233@Configurationpublic class CustomHttpMessageConverter extends AbstractHttpMessageConverter&lt;FoobarVO&gt; &#123; public CustomHttpMessageConverter() &#123; // 添加支持的媒体类型 super(new MediaType("application")); &#125; @Override protected boolean supports(Class&lt;?&gt; clazz) &#123; return true; &#125; @Override protected FoobarVO readInternal(Class&lt;? extends FoobarVO&gt; clazz, HttpInputMessage inputMessage) throws IOException, HttpMessageNotReadableException &#123; // 原始数据 InputStream body = inputMessage.getBody(); // 读取数据 BufferedReader reader = new BufferedReader(new InputStreamReader(body)); StringBuilder stringBuilder = new StringBuilder(); String str; while ((str = reader.readLine()) != null) &#123; stringBuilder.append(str); &#125; stringBuilder.trimToSize(); return JSON.parseObject(stringBuilder.toString(), clazz); &#125; @Override protected void writeInternal(FoobarVO foobarVO, HttpOutputMessage outputMessage) throws IOException, HttpMessageNotWritableException &#123; // 直接输出对象 outputMessage.getBody().write(foobarVO.setTips("Used CustomHttpMessageConverter~").toString().getBytes()); &#125;&#125; 添加到全局消息转换器中 1234@Overridepublic void extendMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; converters.add(new CustomHttpMessageConverter());&#125; 控制台输出com.egovchina.partybuilding.mobile.config.CustomHttpMessageConverter$$EnhancerBySpringCGLIB$$4a5e212f@6ea0dfaa org.springframework.http.converter.ByteArrayHttpMessageConverter@7b7695f org.springframework.http.converter.StringHttpMessageConverter@70a9f6c org.springframework.http.converter.StringHttpMessageConverter@4476ac12 org.springframework.http.converter.ResourceHttpMessageConverter@447522b4 org.springframework.http.converter.ResourceRegionHttpMessageConverter@3569f0f1 org.springframework.http.converter.xml.SourceHttpMessageConverter@9a88a92 org.springframework.http.converter.support.AllEncompassingFormHttpMessageConverter@784fb189 org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@5bddb619 org.springframework.http.converter.json.MappingJackson2HttpMessageConverter@15c25cb6 org.springframework.http.converter.xml.Jaxb2RootElementHttpMessageConverter@6fd6d887 控制台输出]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录使用Arrays.asList()时异常事故]]></title>
    <url>%2Fposts%2F26f10a8%2F</url>
    <content type="text"><![CDATA[一、问题背景&ensp; &ensp; 项目中新的需求，后台登录需要限制非党务工作者进入。首先想到的解决思路，先构造一个能登入后台的账号类型集合并赋予初始值(名称为canLoginAccountTypes)，并把当前登录人的账号类型添加到这个集合中去，然后对canLoginAccountTypes集合进行去重，用去重之后的集合大小跟原本的集合大小进行对比，如果去重之后的集合size()小于原本集合的size()的话表示这个账号属于能登录的账号类型，相反不能登录。（可能有人会问为啥不直接用contains()或containsAll()方法，不使用contains()原因是因为账号的类型可能有多个，不使用containsAll()原因是我只需要匹配一个值即可。）伪代码如下： 二、产生异常&amp;分析异常 &ensp; &ensp; 从报错信息中看出异常产生自checkIsOrdinaryAccount第二行使用的addAll()方法，这个异常从字面意思上很好里面UnsupportedOperation(不支持的异常)。 先看当前调用的类及方法 Arrays.class asList() 123public static &lt;T&gt; List&lt;T&gt; asList(T... a) &#123; return new ArrayList&lt;&gt;(a);&#125; 可以明显的看到当我们使用asList()这个方法时，内部已经是new了一个名称叫做ArrayList的类。值得注意的是这个ArrayList并非java.util下的类，而是java.util.Arrays下的内部类。以下来分析这两个类的差异 java.util.ArrayList 1234567891011public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; // 忽略该类其他方法 重点关注Arrays这个类&#125; java.util.ArrayList 123private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements RandomAccess, java.io.Serializable &#123; // 暂且省略&#125; 仔细对比不难发现，这两个类的共同点都是继承自AbstractList这个父类。不同点是Arrays.ArrayList并没有实现addAll()这个方法，那么我这个canLoginAccountTypes这个集合到底使用哪个类的方法呢？继续从异常信息中入手，明显的可以看到原来是使用了java.util.AbstractCollection.addAll()这个方法，原来调用的是父类的父类 123456789public abstract class AbstractCollection&lt;E&gt; implements Collection&lt;E&gt; &#123; public boolean addAll(Collection&lt;? extends E&gt; c) &#123; boolean modified = false; for (E e : c) if (add(e)) modified = true; return modified; &#125;&#125; 可以看到上述代码addAll()方法内部实现，实现又引用add()这个方法，那么我们可以继续的看下这个add()方法（该方法也属于AbstractCollection类）的。至此异常就非常明显了： 123public boolean add(E e) &#123; throw new UnsupportedOperationException();&#125; 三、最终解决方案]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈API安全及版本控制]]></title>
    <url>%2Fposts%2F03b1a11%2F</url>
    <content type="text"><![CDATA[一、前言&ensp; &ensp; 数据安全是互联网永恒的话题，API直接基于公开的外网请求必将产生，数据被抓包窃取、数据被篡改、数据被大规模的恶意调用等问题。如系统形成完整的产品线，接口的调用方可能有PC端、APP端、第三方系统调用，这时我们必须做好API的版本控制，才能保证一套API能满足所有形式的调用。 二、API安全安全的数据传输应该建立在https这种安全的协议上。我认为一个安全的API系统，应该满足如下要求： 使用令牌方式进行身份验证 使用网关黑白名单控制恶意调用和限流 使用接口幂等控制重复提交 使用加签名防止篡改数据 敏感数据全部基于SSL 第三方系统调用必须遵循OAuth2 三、API版本控制&ensp; &ensp; 随着业务的快速迭代，设计API时应考虑向后兼容。一般的做法就是在原来的数据响应结构上添加，多次迭代后响应出去的数据将会越来越多，就会产生大量的冗余，调用方根本不知道那个版本该用哪些数据，避免不了跟提供方进行没必要的沟通，造成开发效率低下。 针对该问题产生如下解决方法： 3.1 客户端 1）基于URL URL中指定版本号 GET https://www.api.com/SERVER_NAME/v1/users 如果接口版本大部分都不统一，可以采用这种方式，接口版本统一也可使用，只是所有接口都需要加上版本号，服务端直接根据请求url跳转对对应的接口。 参数中指定版本号 GET https://www.api.com/SERVER_NAME/users?version=1 该处理方法虽能直观的体现版本，但是不利于版本迭代，每次资源的变化就得变化url，并且也违背了RESTful API风格。 2）基于Request Headers Header中自定义参数 Api-Version: 1 该处理方法最大的优点是保证了url统一，调用方只要传指定数据请求不同版本的数据，如果未指定提供方可默认响应最新版本数据。 3）基于Token 数据库中保存客户端名、默认的版本号、特殊接口uri、特殊接口uri地址，废弃uri、废弃uri版本。 前端不需要关注版本号，版本控制完全由后台控制，如A系统用户登录，服务器将系统的默认的版本号、特殊uri及版本号、废弃uri及版本号存放在token中，客户端发起请求时先对token进行校验，校验通过后先根据token和uri先判断调用uri和版本号是否废弃，废弃则提示废弃，提示uri应跳转的版本号；然后是否在特殊版本接口中，是跳转对应版本接口，否则跳转到默认版本的接口中；对于不需要token的接口则默认跳转到最新版本的接口中或者客户端在head中存放系统名，跳转到系统默认的版本接口中。]]></content>
      <categories>
        <category>解决方案</category>
      </categories>
      <tags>
        <tag>解决方案</tag>
      </tags>
  </entry>
</search>
